{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extracts spike and neuron data from the HDF5 file and saves it in csv files. The following files are created:\n",
    "* <b>neurons.csv:</b> contains x and y coordinates of neurons, row index in the csv == neuron index\n",
    "* <b>allSpikeTime.csv:</b> contains the following columns: spike timestep, neuron id, xloc, yloc (coordinates of the spiking neuron, starter (1 if the spiking neuron is a starter neuron, 0 otherwise; type: int).\n",
    "* <b>allSpikeTimeCount.csv:</b> contains spike timesteps and count of spikes per timestep\n",
    "\n",
    "Author: Mariia Lundvall (lundvm@uw.edu) <br>\n",
    "Date: 07/04/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import h5py\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gc\n",
    "import numpy.ma as ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> NOTE: Update the paths prior to running this notebook. <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the HDF5 file\n",
    "hdf5_path = '/home/NETID/lundvm/data/tR_1.0--fE_0.90.h5'\n",
    "# path where to save a file with neuron data\n",
    "neurons_csv = '/home/NETID/lundvm/data/neurons_test_070419.csv'\n",
    "# path where to save a file with spike data\n",
    "spikes_csv = '/home/NETID/lundvm/data/allSpikeTime_test_070419.csv'\n",
    "# path to where save a file with spike counts\n",
    "spike_count_csv = '/home/NETID/lundvm/data/allSpikeTimeCount_test_070419.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spike_matrix(f):\n",
    "    \"\"\"\n",
    "    Takes in an HDF5 file produced by the BrainGrid simuation, extracts information about time \n",
    "    and location of spikes and converts it into a dataframe with the following columns: time_step \n",
    "    (time step when the spike happened, type: int), id (neuron id, type: int), xloc, yloc \n",
    "    (coordinates of the spike, type: int), starter (1 if the spiking neuron is a starter neuron, 0 otherwise; type: int).\n",
    "    \n",
    "    Args: \n",
    "        f(HDF5): loaded HDF5 file to read data from.\n",
    "    \n",
    "    Returns:\n",
    "        spikes_loc(pandas dataframe): dataframe that contains spiking data. The dataframe has the \n",
    "            following columns: time_step (time step when the spike happened, type: int), id (neuron id, type: int), \n",
    "            xloc, yloc (coordinates of the spike, type: int), \n",
    "            starter (1 of the neuron is a starter neuron, 0 otherwise; type: int).\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the spikes time steps and coordinates data from the hdf5 file and convert the data \n",
    "    # into numpy arrays\n",
    "    spikes = np.array(f['/spikesProbedNeurons'], dtype='uint32')\n",
    "    xloc = np.array(f['/xloc'], dtype='uint8')\n",
    "    yloc = np.array(f['/yloc'], dtype='uint8')\n",
    "    idx = np.array(f['/starterNeurons'], dtype='uint16')\n",
    "    ids = np.array(f['/probedNeurons'], dtype='uint16')\n",
    "    starter = np.zeros((10000,))\n",
    "    starter[idx] = 1\n",
    "    # m is the max number of spikes per neuron, n is the number of neurons in the simulation\n",
    "    m, n = spikes.shape\n",
    "    # transform the spikes matrix:\n",
    "    # 1. Traspose so that each row is a sequence of spikes of one neuron (instead of a column)\n",
    "    # 2. Flattem the matrix \n",
    "    # 3. Reshape into a 2d array from (m*n, ) to (m*n, 1). This is needed for further processing. \n",
    "    spikes = np.transpose(spikes).flatten().reshape(m*n, 1)\n",
    "    # create a mask to remove non-spikes (where time step=0)\n",
    "    mask = ma.masked_equal(spikes, 0).reshape(m*n, )\n",
    "    # Transform the coordinate vectors:\n",
    "    # 1. Make the vectors match the time step sequence. Repeat the values so that first m values \n",
    "    # in the x and y vectors are the coordinates of the first neuron\n",
    "    # 2. Remove the coordinates corresponding to non-spikes\n",
    "    # 3. Concatenate x and y, the result is an array xy of shape (m*n, 2)\n",
    "    xloc = np.compress(mask, np.repeat(xloc, m).reshape(m*n, 1), axis=0)\n",
    "    yloc = np.compress(mask, np.repeat(yloc, m).reshape(m*n, 1), axis=0)\n",
    "    xy = np.concatenate((xloc, yloc), axis=1)\n",
    "    # delete xloc and yloc to free memory\n",
    "    del xloc\n",
    "    del yloc\n",
    "    gc.collect()\n",
    "    starter = np.compress(mask, np.repeat(starter, m).reshape(m*n, 1), axis=0)\n",
    "    ids = np.compress(mask, np.repeat(ids, m).reshape(m*n, 1), axis=0)\n",
    "    t = np.concatenate((ids, starter), axis=1)\n",
    "    xyis = np.concatenate((xy, t), axis=1)\n",
    "    del xy\n",
    "    del t\n",
    "    del starter \n",
    "    del ids\n",
    "    gc.collect()\n",
    "    # Remove non-spikes from the time step array, concatenate it to xy, \n",
    "    # and convert the result into a dataframe\n",
    "    spikes_loc = pd.DataFrame(np.concatenate((np.compress(mask, spikes, axis=0), xyis), axis=1))\n",
    "    # delete spikes to free the memory\n",
    "    del spikes\n",
    "    gc.collect()\n",
    "    spikes_loc.rename(columns={0:'time_step', 1:'xloc', 2:'yloc', 3:'id', 4:'starter'}, inplace=True)\n",
    "    spikes_loc.sort_values(by='time_step', inplace=True)\n",
    "    spikes_loc.reset_index(drop=True, inplace=True)\n",
    "    spikes_loc['time_step'] = spikes_loc['time_step'].astype('int32')\n",
    "    spikes_loc['id'] = spikes_loc['id'].astype('int16')\n",
    "    spikes_loc['xloc'] = spikes_loc['xloc'].astype('uint8')\n",
    "    spikes_loc['yloc'] = spikes_loc['yloc'].astype('uint8')\n",
    "    spikes_loc['starter'] = spikes_loc['starter'].astype('uint8')\n",
    "    \n",
    "    return spikes_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tsim', 'burstinessHist', 'neuronThresh', 'neuronTypes', 'probedNeurons', 'radiiHistory', 'ratesHistory', 'simulationEndTime', 'spikesHistory', 'spikesProbedNeurons', 'starterNeurons', 'xloc', 'yloc'] \n",
      "\n",
      "[<HDF5 dataset \"Tsim\": shape (1,), type \"<f4\">, <HDF5 dataset \"burstinessHist\": shape (60000,), type \"<i4\">, <HDF5 dataset \"neuronThresh\": shape (10000,), type \"<f4\">, <HDF5 dataset \"neuronTypes\": shape (10000,), type \"<i4\">, <HDF5 dataset \"probedNeurons\": shape (10000,), type \"<i4\">, <HDF5 dataset \"radiiHistory\": shape (601, 10000), type \"<f4\">, <HDF5 dataset \"ratesHistory\": shape (601, 10000), type \"<f4\">, <HDF5 dataset \"simulationEndTime\": shape (1,), type \"<f4\">, <HDF5 dataset \"spikesHistory\": shape (6000000,), type \"<i4\">, <HDF5 dataset \"spikesProbedNeurons\": shape (375898, 10000), type \"<u8\">, <HDF5 dataset \"starterNeurons\": shape (1000,), type \"<i4\">, <HDF5 dataset \"xloc\": shape (10000,), type \"<i4\">, <HDF5 dataset \"yloc\": shape (10000,), type \"<i4\">]\n"
     ]
    }
   ],
   "source": [
    "#load an hdf5 file with the simulation data\n",
    "f = h5py.File(hdf5_path, 'r')\n",
    "print(list(f.keys()), '\\n')\n",
    "print(list(f.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 31s, sys: 2min 26s, total: 9min 57s\n",
      "Wall time: 7min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create the spiking data matrix\n",
    "spikes_loc = create_spike_matrix(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all starter neurons (optional)\n",
    "# spikes_loc = spikes_loc[spikes_loc['starter'] == 0]\n",
    "# drop 'xloc', 'yloc', and 'starter' columns (optional, update column names as necessary)\n",
    "# spikes_loc.drop(columns=['xloc', 'yloc', 'starter'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save spike data to a csv\n",
    "spikes_loc.to_csv(spikes_csv, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract neuron data and save it to a csv\n",
    "xloc = np.array(f['/xloc'], dtype='uint8').reshape(10000,1)\n",
    "yloc = np.array(f['/yloc'], dtype='uint8').reshape(10000,1)\n",
    "xy = np.concatenate((xloc, yloc), axis=1)\n",
    "neurons = pd.DataFrame(xy)\n",
    "neurons = neurons.astype(int)\n",
    "np.savetxt(neurons_csv, neurons, fmt='%i', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with counts of spikes per timestep\n",
    "temp1 = np.bincount(spikes_loc['time_step'].values)\n",
    "temp2 = np.nonzero(temp1)[0]\n",
    "spikes_time_count = np.vstack((temp2,temp1[temp2])).T\n",
    "spike_count_df = pd.DataFrame(spikes_time_count)\n",
    "# save the dataframe to a csv\n",
    "spike_count_df.to_csv(spike_count_csv, header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
